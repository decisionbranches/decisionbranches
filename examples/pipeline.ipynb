{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/decisionbranches/decisionbranches/blob/master/examples/pipeline.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Needles in Massive Haystacks: Fast Search-By-Classification in Large-Scale Data Catalogs - Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/decisionbranches/decisionbranches/raw/main/figures/pipeline.png\" alt=\"Drawing\" style=\"width: 400px;\"/> \\\n",
    "This notebook guides through our whole search pipeline as shown in the figure. We use the **Satimage dataset** for illustrating how to use the pipeline. Therefore, no feature extraction of the data is required in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download and install our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/decisionbranches/decisionbranches.git\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from decisionbranches.utils.helpers import generate_fidxs\n",
    "from decisionbranches.models.boxSearch.boxClassifier import BoxClassifier\n",
    "from py_kdtree.treeset import KDTreeSet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=42\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "#Parameter\n",
    "nfeat = 10 #Size of feature subsets \n",
    "nind = 100 #Number of feature subsets\n",
    "dbranch_cfg = {\"top_down\":False,\"max_evals\":\"all\",\"stop_infinite\":True} \n",
    "#top_down: if top-down refinement phase should be enabled; max_evals: number of feature subsets to evaluate per iteration; stop_infinite: stop expansion until infinity\n",
    "\n",
    "label = \"4.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 1: Offline Preprocessing\n",
    "In this preprocessing phase, we prepare the indexes required for our classifier. Note that this phase may take significantly longer for larger datasets but it only needs to be executed **once** to make our classifier operable. Afterwards, multiple queries can be executed without any additional preprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load features\n",
    "We load the Satimage dataset from OpenML (https://www.openml.org/) and transform the dataset into a binary classification problem to make it useable for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = fetch_openml('satimage', version=1, return_X_y=True, as_frame=False) #Load dataset from OpenML database\n",
    "\n",
    "y_bin = np.zeros(len(y),dtype=int)\n",
    "y_bin[y==label] = 1 #Make binary classification problem out of the dataset\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y_bin,train_size=0.05,random_state=seed) #Split data into train and test set\n",
    "print(\"Number of rare training objects: \",np.sum(y_train))\n",
    "print(\"Number of points to query: \",len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate feature subsets (indexes)\n",
    "We randomly generate the required feature subsets in this case but they can be also indiviually specified if required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsets = generate_fidxs(n_feat=nfeat,n_ind=nind,feats=np.arange(X.shape[1]),seed=seed) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build indexes\n",
    "Multiple indexes are built based on the generated feature subsets from the step before. The *treeset* object abstracts all index structures for the user at once. The individual indexes (kd-trees) are stored under *path*. Under *path* each kd-tree is identified via the ids of the corresponding feature subset. For each kd-tree the tree structure (.pkl file) and their leaves (.mmap file) are stored.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "treeset = KDTreeSet(subsets,path=\"./indexes/\",leaf_size=60,verbose=False)\n",
    "treeset.fit(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phase 2: Query Processing\n",
    "In this phase, we simulate an examplary user query to find rare objects using our index-aware classifier. The user query consists of labeled rare (y=1) and non-rare (y=0) instances. In this case the labeled instances of the user query are contained in *X_train* (features) and *y_train* (labels). The data catalog on which the search is executed is represented by *X_test* and *y_test*. Note that in practice the data catalog should be many times larger to take full advantage of our search-by-classification approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Index-aware Classifier\n",
    "We provide our classifier (BoxClassifier) the indexes in which to perform the search via the parameter *indices*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbranch = BoxClassifier(tot_feat=X.shape[1],indices=subsets,cfg=dbranch_cfg)\n",
    "\n",
    "dbranch.fit(X_train,y_train) #Construct the boxes\n",
    "\n",
    "preds = dbranch.predict(X_test)\n",
    "print(\"Test F1-score: \",f1_score(y_test, preds)) # Search quality of our found boxes in the unknown data catalog"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract range queries (boxes)\n",
    "We extract the found boxes. These are defined by their minimum point, maximum point and corresponding feature subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins,maxs,fidxs = dbranch.get_boxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query boxes\n",
    "We query for the found boxes in the index structures. The function *multi_query_ranked_cy* performs the search for all boxes at once and sorts the found point based on their number of occurences (points that are contained in multiple boxes are thereby counted)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds,counts,time,loaded_leaves = treeset.multi_query_ranked_cy(mins,maxs,fidxs)\n",
    "\n",
    "print(\"Number of found points: \",len(inds))\n",
    "print(\"Loading time: \",time)\n",
    "print(\"Number of loaded leaves: \",loaded_leaves)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble\n",
    "The same query is repeated for our ensemble classifier which consists of 25 classifiers. We can observe that the search quality increases at the cost of longer query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decisionbranches.models.boxSearch.ensemble import Ensemble\n",
    "ens = Ensemble(tot_feat=X.shape[1],indices=subsets,n_estimators=25,cfg={\"max_evals\":\"auto\"}) #n_estimators: number of models in the ensemble\n",
    "\n",
    "ens.fit(X_train,y_train)\n",
    "\n",
    "preds = ens.predict(X_test)\n",
    "print(\"Test F1-score: \",f1_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract range queries (boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mins,maxs,fidxs = ens.get_boxes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inds,counts,time,loaded_leaves = treeset.multi_query_ranked_cy(mins,maxs,fidxs)\n",
    "\n",
    "print(\"Number of found points: \",len(inds))\n",
    "print(\"Loading time: \",time)\n",
    "print(\"Number of loaded leaves: \",loaded_leaves)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
